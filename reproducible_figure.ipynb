{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.17.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import datetime as dt\n",
    "from pandas_datareader import data as pdr\n",
    "import plotly.offline as plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import yfinance as yf\n",
    "yf.pdr_override()\n",
    "import os\n",
    "plotly.init_notebook_mode(connected=True)\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "import wrds\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.stattools import acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_close_clean = pd.read_csv('data/df_close_clean.csv')\n",
    "df_open_clean = pd.read_csv('data/df_open_clean.csv')\n",
    "df_volume_clean = pd.read_csv('data/df_volume_clean.csv')\n",
    "df_shares_outstanding_clean = pd.read_csv('data/df_shares_outstanding_clean.csv')\n",
    "df_volume_clean_wrds = pd.read_csv('data/df_volume_clean_WRDS.csv')\n",
    "\n",
    "# change index to datetime\n",
    "df_close_clean.index = pd.to_datetime(df_close_clean['Date'])\n",
    "df_open_clean.index = pd.to_datetime(df_open_clean['Date'])\n",
    "df_volume_clean.index = pd.to_datetime(df_volume_clean['Date'])\n",
    "df_shares_outstanding_clean.index = pd.to_datetime(df_shares_outstanding_clean['Date'])\n",
    "df_volume_clean_wrds.index = pd.to_datetime(df_volume_clean_wrds['Date'])\n",
    "\n",
    "# Remove the \"Date\" column as it no longer has any use\n",
    "df_close_clean = df_close_clean.drop(columns=['Date'])\n",
    "df_open_clean = df_open_clean.drop(columns=['Date'])\n",
    "df_volume_clean = df_volume_clean.drop(columns=['Date'])\n",
    "df_shares_outstanding_clean = df_shares_outstanding_clean.drop(columns=['Date'])\n",
    "df_volume_clean_wrds = df_volume_clean_wrds.drop(columns=['Date'])\n",
    "# df_close_clean = df_close_clean.drop(columns=['Date.1'])\n",
    "# df_open_clean = df_open_clean.drop(columns=['Date.1'])\n",
    "# df_volume_clean = df_volume_clean.drop(columns=['Date.1'])\n",
    "df_shares_outstanding_clean = df_shares_outstanding_clean.drop(columns=['Date.1'])\n",
    "df_volume_clean_wrds = df_volume_clean_wrds.drop(columns=['Date.1'])\n",
    "\n",
    "# testing if the index is datetime and the \"Date\" column is removed\n",
    "assert 'Date' not in df_close_clean.columns\n",
    "assert 'Date' not in df_open_clean.columns\n",
    "assert 'Date' not in df_volume_clean.columns\n",
    "assert 'Date' not in df_shares_outstanding_clean.columns\n",
    "assert 'Date' not in df_volume_clean_wrds.columns\n",
    "assert df_close_clean.index.dtype == 'datetime64[ns]'\n",
    "assert df_open_clean.index.dtype == 'datetime64[ns]'\n",
    "assert df_volume_clean.index.dtype == 'datetime64[ns]'\n",
    "assert df_shares_outstanding_clean.index.dtype == 'datetime64[ns]'\n",
    "assert df_volume_clean_wrds.index.dtype == 'datetime64[ns]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_open_clean = pd.read_csv('data/df_opening_SnP500_39_years_clean.csv', parse_dates=True, index_col='Date')\n",
    "df_close_clean = pd.read_csv('data/df_closing_SnP500_39_years_clean.csv', parse_dates=True, index_col='Date')\n",
    "df_open_clean = df_open_clean.drop(columns ='Date.1')\n",
    "df_close_clean = df_close_clean.drop(columns ='Date.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_open_clean = pd.read_csv('data/df_open_clean.csv', parse_dates=True, index_col='Date')\n",
    "df_close_clean = pd.read_csv('data/df_close_clean.csv', parse_dates=True, index_col='Date')\n",
    "df_open_clean = df_open_clean.drop(columns ='Index')\n",
    "df_close_clean = df_close_clean.drop(columns ='Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_open_clean.index.dtype == 'datetime64[ns]'\n",
    "assert df_close_clean.index.dtype == 'datetime64[ns]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation of returns following feng et al. (2012) method:\n",
    "df_real_returns_log_difference = df_open_clean.apply(np.log) - df_close_clean.apply(np.log) \n",
    "\n",
    "# normalising the returns\n",
    "df_real_returns_log_difference = (df_real_returns_log_difference - df_real_returns_log_difference.mean()) / df_real_returns_log_difference.std()\n",
    "\n",
    "# taking absolute of real returns\n",
    "df_real_returns_log_difference_abs = df_real_returns_log_difference.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening dataframes\n",
    "real_returns_log_difference_abs_flattened = df_real_returns_log_difference_abs.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_log = sorted(real_returns_log_difference_abs_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other parameters\n",
    "n = 2**10\n",
    "t = 10000\n",
    "p = 0.02178\n",
    "omega = 1\n",
    "b = 1\n",
    "size = 1\n",
    "k = 1.5\n",
    "M = 500\n",
    "d = 1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent behaviors for buying, selling and holding based on the probability of trading p\n",
    "def buy_sell_hold(p, amount_times):\n",
    "    assert p <= 0.5, \"p should be smaller than 0.5\"\n",
    "    psis = np.zeros(amount_times)\n",
    "    dice_rolls = np.random.uniform(0, 1, amount_times)\n",
    "    indices = np.nonzero(dice_rolls <= 2*p)\n",
    "\n",
    "    for index in indices[0]:\n",
    "        if np.random.randint(0,2) == 0:\n",
    "            psis[index] = 1\n",
    "        else:\n",
    "            psis[index] = -1\n",
    "    return psis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent model w noise\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, p: float, n: int, size: int, k: int, omega: int, b: int) -> None:\n",
    "        self.n = n\n",
    "        self.p = p\n",
    "        self.daily_return = 1\n",
    "        self.trading_volume = 0\n",
    "        self.k = k\n",
    "        self.omega = omega\n",
    "        self.daily_returns = []\n",
    "        self.count = 0\n",
    "        self.b = b\n",
    "\n",
    "        self.daily_trading_volumes = []\n",
    "\n",
    "        \n",
    "    def distribute_opinion_groups(self) -> int:\n",
    "        if self.b == 0:\n",
    "            # number of opinion groups\n",
    "            c = (self.n / abs(self.daily_return)) ** self.omega\n",
    "                \n",
    "        else:\n",
    "            mean = (self.n / abs(self.daily_return)) ** self.omega\n",
    "\n",
    "            \n",
    "            c = np.random.normal(loc = mean, scale = math.sqrt(mean * self.b))\n",
    "            c = int(np.round(c))\n",
    "            # because logically there can only be a minimum of 1 opinion group, i.e. all agents have the same opinion, we set the minimum c to be 1.\n",
    "            if c <= 0:\n",
    "                c = 1\n",
    "            if abs(self.daily_return) >= self.n:\n",
    "                c = 1\n",
    "        # rounding, as we can only have whole number of opinion groups\n",
    "        c = int(np.round(c))\n",
    "\n",
    "        # number of opnion groups can maximally be the number of agents\n",
    "        c = np.min([c, self.n])\n",
    "            \n",
    "        return c\n",
    "\n",
    "    def step(self):\n",
    "        self.count += 1\n",
    "        c = self.distribute_opinion_groups()\n",
    "        psis = buy_sell_hold(self.p, c)\n",
    "        average_agents_per_group = int(np.round(self.n/c))\n",
    "        return_matrix = psis * average_agents_per_group\n",
    "        trading_volume = np.sum(np.abs(return_matrix))\n",
    "        self.daily_return = np.sum(return_matrix) #* k\n",
    "         # following the boundry conditions on returns from Feng et al. 2012 appendix 5:\n",
    "        minimum_return = self.n**((self.omega-1)/self.omega)\n",
    "        if np.abs(self.daily_return) < minimum_return:\n",
    "            sign = -1 if self.daily_return < 0 else 1\n",
    "            self.daily_return = sign*minimum_return\n",
    "        maximum_return = self.n\n",
    "        if np.abs(self.daily_return) > maximum_return:\n",
    "            self.daily_return = np.sign(self.daily_return)*maximum_return\n",
    "        self.daily_returns.append(self.daily_return)\n",
    "        self.daily_trading_volumes.append(trading_volume)\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stochastic_Model:\n",
    "    def __init__(self, n: int, p: float, init: int, time_horizon: bool, M, d):\n",
    "        self.returns = [init]\n",
    "        self.n = n\n",
    "        self.p = p\n",
    "        self.time_horizon = time_horizon\n",
    "        self.M = M\n",
    "        self.d = d\n",
    "\n",
    "    def time_horizons(self):\n",
    "        time_horizons = []\n",
    "        alpha = []\n",
    "\n",
    "        # distribution of agents in different time horizons\n",
    "        # agents are allocated to each time horizon (with exponential decay as we go back in time), until we reach the limit M\n",
    "        # or until we reach the beginning of the simulation (if current timestep t < M)\n",
    "        for i in range(1, self.M + 1):\n",
    "            if len(self.returns) == 1:\n",
    "                value = (i ** -self.d) * abs(self.returns[-1])\n",
    "                alpha_val = (i ** -self.d)\n",
    "                time_horizons.append(value)\n",
    "                alpha.append(alpha_val)\n",
    "\n",
    "            elif i >= len(self.returns):\n",
    "                value = (i ** -self.d) * abs(self.returns[-1] - self.returns[0])\n",
    "                alpha_val = (i ** -self.d)\n",
    "                time_horizons.append(value)\n",
    "                alpha.append(alpha_val)\n",
    "\n",
    "            else:\n",
    "                value = (i ** -self.d) * abs(self.returns[-1] - self.returns[-1-i])\n",
    "                alpha_val = (i ** -self.d)\n",
    "                time_horizons.append(value)\n",
    "                alpha.append(alpha_val)\n",
    "\n",
    "        return sum(time_horizons) * (sum(alpha) ** -1)\n",
    "\n",
    "    def step(self):\n",
    "\n",
    "        # Agents only factor in the returns at the previous timestep\n",
    "        if self.time_horizon == False:\n",
    "            variance  = 2 * p * n * abs(self.returns[-1])\n",
    "            std = math.sqrt(variance)\n",
    "            value = std * np.random.normal(0.0, 1.0)\n",
    "            self.returns.append(value)\n",
    "        \n",
    "        # Agents look back M timesteps\n",
    "        if self.time_horizon == True:\n",
    "            horizons = self.time_horizons()\n",
    "            variance  = 2 * p * n * horizons\n",
    "            std = math.sqrt(variance)\n",
    "            value = std * np.random.normal(0.0, 1.0)\n",
    "            self.returns.append(value)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(returns):\n",
    "    \"\"\"\n",
    "    Normalises an array\n",
    "    \"\"\"\n",
    "    array = np.ravel(returns)\n",
    "    return np.abs((array - array.mean())/array.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_abm = Model(p, n, 1, k, omega, b)\n",
    "for i in range(n):\n",
    "    ks_abm.step()\n",
    "\n",
    "abm_returns = normalise(ks_abm.daily_returns)\n",
    "\n",
    "ks_stoch = Stochastic_Model(p, n, 1, False, M, d)\n",
    "for i in range(n):\n",
    "    ks_stoch.step()\n",
    "\n",
    "stoch_returns = normalise(ks_stoch.returns)\n",
    "\n",
    "ks_stoch_horizon = Stochastic_Model(p, n, 1, True, M, d)\n",
    "for i in range(n):\n",
    "    ks_stoch_horizon.step()\n",
    "\n",
    "stoch_horizon_returns = normalise(ks_stoch_horizon.returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_returns = [abm_returns, stoch_returns, stoch_horizon_returns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bin = np.logspace(0,3, 100)\n",
    "plt.figure(1)\n",
    "for data_return in data_returns:\n",
    "    plt.hist(data_return, bins = bin, log=True, density = True, cumulative=-1, alpha=0.3, histtype='step')\n",
    "plt.hist(sorted_log, bins = bin, log=True, density = True, cumulative=-1, alpha=1, histtype='step', color='red')\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('cumulative_distribution.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
